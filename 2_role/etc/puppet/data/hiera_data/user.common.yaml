
#### Build Node Cobbler Information ######## 

# If you use an HTTP/HTTPS proxy to reach the apt repositories that
# packages will be installed from during installation, uncomment this
# setting and specify the correct proxy URL.  If you do not use an
# HTTP/HTTPS proxy, leave this setting commented out.
#proxy: 'http://proxy-server.domain.com:8080'

#### Disk partitioning options ###########
# The /var directory is where logfiles and instance data are stored
# on disk.  If you wish to have /var on it's own partition (considered
# a best practice), set enable_var to true.
enable_var: true

# The Cinder volume service can make use of unallocated space within
# the "cinder-volumes" volume group to create iscsi volumes for 
# export to instances.  If you wish to leave free space for volumes
# and not preallocate the entire install drive, set enable_vol_space
# to true.
enable_vol_space: true

# Use the following two directives to set the size of the / and /var
# partitions, respectively.  The var_part_size directive will be ignored
# if enable_var is not set to true above.
root_part_size: 65536
var_part_size: 432000

# This domain name will be the name your build and compute nodes use for the
# local DNS.  It doesn't have to be the name of your corporate DNS - a local
# DNS server on the build node will serve addresses in this domain - but if
# it is, you can also add entries for the nodes in your corporate DNS
# environment they will be usable *if* the above addresses are routeable
# from elsewhere in your network.
domain_name: domain.name

########### NTP Configuration ############
# Change this to the location of a time server or servers in your
# organization accessible to the build server.  The build server will
# synchronize with this time server, and will in turn function as the time
# server for your OpenStack nodes.
ntp_servers:
  - 1.pool.ntp.org

# The time zone that clocks should be set to.  See /usr/share/zoneinfo
# for valid values, such as "UTC" and "US/Eastern".
time_zone: UTC

######### Node Addresses ##############
# Change the following to the short host name you have given your build node.
# This name should be in all lower case letters due to a Puppet limitation
# (refer to http://projects.puppetlabs.com/issues/1168).
build_node_name: build-server

# Change the following to the host name you have given to your control
# node.  This name should be in all lower case letters due to a Puppet
# limitation (refer to http://projects.puppetlabs.com/issues/1168).
coe::base::controller_hostname: control-server

# The IP address to be used to connect to Horizon and external
# services on the control node.  In the compressed_ha or full_ha scenarios,
# this will be an address to be configured as a VIP on the HAProxy
# load balancers, not the address of the control node itself.
controller_public_address: 192.168.255.191

# The protocol used to access API services on the control node.
# Can be 'http' or 'https'.
controller_public_protocol: 'http'

# The IP address used for internal communication with the control node.
# In the compressed_ha or full_ha scenarios, this will be an address
# to be configured as a VIP on the HAProxy load balancers, not the address
# of the control node itself.
controller_internal_address: 192.168.255.191

# The IP address used for management functions (such as monitoring)
# on the control node.  In the compressed_ha or full_ha scenarios, this will
# be an address to be configured as a VIP on the HAProxy
# load balancers, not the address of the control node itself.
controller_admin_address: 192.168.255.191

# Control node interfaces.
# internal_ip be used for the ovs local_ip setting for GRE tunnels.

# This sets the IP for the private(internal) interface of controller nodes
# (which is predefined already in $controller_node_internal, and the internal
# interface for compute nodes.  It is generally also the IP address
# used in Cobbler node definitions.
internal_ip: "%{ipaddress_eth0}"

# The external_interface is used to provide a Layer2 path for
# the l3_agent external router interface.  It is expected that
# this interface be attached to an upstream device that provides
# a L3 router interface, with the default router configuration
# assuming that the first non "network" address in the external
# network IP subnet will be used as the default forwarding path
# if no more specific host routes are added.
external_interface: eth1

# The public_interface will have an IP address reachable by
# all other nodes in the openstack cluster.  This address will
# be used for API Access, for the Horizon UI, and as an endpoint
# for the default GRE tunnel mechanism used in the OVS network
# configuration.
public_interface: eth0

# The interface used for VM networking connectivity.  This will usually
# be set to the same interface as public_interface.
private_interface: eth0

### Cobbler config
# The IP address of the node on which Cobbler will be installed and
# on which it will listen.
cobbler_node_ip: 192.168.255.194

# The subnet address of the subnet on which Cobbler should serve DHCP
# addresses.
node_subnet: '192.168.255.0'

# The netmask of the subnet on which Cobbler should serve DHCP addresses.
node_netmask: '255.255.255.0'

# The default gateway that should be provided to DHCP clients that acquire
# an address from Cobbler.
node_gateway: '192.168.255.1'

# The admin username and crypted password used to authenticate to Cobbler.
admin_user: localadmin
password_crypted: $6$UfgWxrIv$k4KfzAEMqMg.fppmSOTd0usI4j6gfjs0962.JXsoJRWa5wMz8yQk4SfInn4.WZ3L/MCt5u.62tHDGB36EhiKF1

# Cobbler can instruct nodes being provisioned to start a Puppet agent
# immediately upon bootup.  This is generally desirable as it allows
# the node to immediately begin configuring itself upon bootup without
# further human intervention.  However, it may be useful for debugging
# purposes to prevent Puppet from starting automatically upon bootup.
# If you want Puppet to run automatically on bootup, set this to true.
# Otherwise, set it to false.
autostart_puppet: true

# If you are using Cisco UCS servers managed by UCSM, set the port on
# which Cobbler should connect to UCSM in order to power nodes off and on.
# If set to 443, the connection will use SSL, which is generally
# desirable and is usually enabled on UCS systems.
ucsm_port: 443

# The name of the hard drive on which Cobbler should install the operating
# system.
install_drive: /dev/sda

# Set to 1 to enable ipv6 route advertisement.  Otherwise, comment out
# this line or set it to 0.
#ipv6_ra: 1

# Uncomment this line and set it to true if you want to use bonded
# ethernet interfaces.
#interface_bonding: true

# The IP address on which vncserver proxyclient should listen.
# This should generally be an address that is accessible via
# horizon.  You can set it to an actual IP address (e.g. "192.168.1.1"),
# or use facter to get the IP address assigned to a particular interface.
nova::compute::vncserver_proxyclient_address: "%{ipaddress_eth0}"

### The following are passwords and usernames used for
### individual services.  You may wish to change the passwords below
### in order to better secure your installation.
cinder_db_password: cinder_pass
glance_db_password: glance_pass
keystone_db_password: key_pass
nova_db_password: nova_pass
network_db_password:   quantum_pass
database_root_password: mysql_pass
cinder_service_password: cinder_pass
glance_service_password: glance_pass
nova_service_password: nova_pass
ceilometer_service_password: ceilometer_pass
admin_password: Cisco123
admin_token: keystone_admin_token
network_service_password: quantum_pass
rpc_password: openstack_rabbit_password
metadata_shared_secret: metadata_shared_secret
horizon_secret_key: horizon_secret_key
ceilometer_metering_secret: ceilometer_metering_secret
ceilometer_db_password: ceilometer
heat_db_password: heat
heat_service_password: heat_pass
heat::engine::auth_encryption_key: 'notgood but just long enough i think'

# Set this parameter to use a single secret for the Horizon secret
# key, neutron agents, Nova API metadata proxies, swift hashes,etc.
# This prevents you from needing to specify individual secrets above,
# but has some security implications in that all services are using
# the same secret (creating more vulnerable services if it should be
# compromised).
secret_key: secret

# Set this parameter to use a single password for all the services above.
# This prevents you from needing to specify individual passwords above,
# but has some security implications in that all services are using
# the same password (creating more vulnerable services if it should be
# compromised).
password: password123

### Swift configuration
# The username used to authenticate to the Swift cluster.
swift_service_password: swift_pass

# The password hash used to authenticate to the Swift cluster.
swift_hash: super_secret_swift_hash

# The IP address used by Swift on the control node to communicate with
# other members of the Swift cluster.  In the compressed_ha or full_ha
# scenarios, this will be the address to be configured as a VIP on 
# the HAProxy load balancers, not the address of an individual Swift node.
swift_internal_address: 192.168.242.41

# The IP address which external entities will use to connect to Swift,
# including clients wishing to upload or retrieve objects.  In the
# compressed_ha or full_ha scenarios, this will be the address to
# be configured as a VIP on the HAProxy load balancers, not the address
# of an individual Swift node.
swift_public_address: 192.168.242.41

# The IP address over which administrative traffic for the Swift
# cluster will flow.  In the compressed_ha or full_ha
# scenarios, this will be the address to be configured as a VIP on 
# the HAProxy load balancers, not the address of an individual Swift node.
swift_admin_address: 192.168.242.41

# The interface on which Swift will run data storage traffic.
# This should generally be a different interface than is used for
# management traffic to avoid congestion.
swift_storage_interface: eth0.222

# The IP address to be configured on the Swift storage interface.
swift_local_net_ip: "%{ipaddress_eth0_222}"

# The netmask to be configured on the Swift storage interface.
swift_storage_netmask: 255.255.255.0

# The IP address of the Swift proxy server. This is the address which
# is used for management, and is often on a separate network from
# swift_local_net_ip.
swift_proxy_net_ip: "%{ipaddress_eth0}"

### The following three parameters are only used if you are configuring
### Swift to serve as a backend for the Glance image service.

# Enable Glance to use Ceph RBD as it's backend storage by uncommenting
# the line below.  It can also be set to "file" or "swift".
#glance_backend: rbd

# The key used by Glance to connect to Swift.
glance::backend::swift::swift_store_key: secret_key

# The IP address to which Glance should connect in order to talk
# to the Swift cluster.
glance::backend::swift::swift_store_auth_address: '127.0.0.1'

### Ceph configuration
# The name of the Ceph cluster to be deployed.
ceph_cluster_name: 'ceph'

# The FSID of the Ceph monitor node.  This should take the form
# of a UUID.
ceph_monitor_fsid: 'e80afa94-a64c-486c-9e34-d55e85f26406'

# The shared secret used to connect to the Ceph monitors.  This
# should be a crypted password.
ceph_monitor_secret: 'AQAJzNxR+PNRIRAA7yUp9hJJdWZ3PVz242Xjiw=='

# The short hostname (e.g. 'ceph-mon01', not 'ceph-mon01.domain.com') of
# the initial members of the Ceph monitor set.
mon_initial_members: 'ceph-mon01'

# The short hostname (e.g. 'ceph-mon01', no 'ceph-mon01.domain.com') of
# the primary monitor node.
ceph_primary_mon: 'ceph-mon01'

# The IP address used to connect to the primary monitor node.
ceph_monitor_address: '10.0.0.1'

# Ceph will be deployed using the cephdeploy tool.  This tool requires
# a username and password to authenticate.
ceph_deploy_user: 'cephdeploy'
ceph_deploy_password: '9jfd29k9kd9'

# The name of the network interface used to connect to Ceph nodes.
# This interface will be used to pass traffic between Ceph nodes.
ceph_cluster_interface: 'eth1'

# The subnet on which Ceph intra-cluster traffic will be passed.
ceph_cluster_network: '10.0.0.0/24'

# The interface on which entities that want to import data into or
# extract data from the cluster will connect.
ceph_public_interface: 'eth1'

# The subnet on which external entities will connect to the Ceph cluster.
ceph_public_network: '10.0.0.0/24'

# A list of kernel modules that should be added to /etc/modules
# and thus auto-loaded into the kernel at boot time.  It is recommended
# that you include at least two modules: "8021q" is necessary for vlan
# tagging, and "vhost_net" increases networking performance of instances.
kernel_module_list:
  - 8021q
  - vhost_net

### The following four parameters are used only if you are configuring
### Ceph to be a backend for the Cinder volume service.

# Enable Cinder to use Ceph RBD as it's backend storage by uncommenting
# the line below.  It can also be set to 'iscsi'.
#cinder_backend: rbd

# The username Cinder should use to connect to the cluster.
cinder_rbd_user: 'admin'

# The name of the pool used to store Cinder volumes.`
cinder_rbd_pool: 'volumes'

# The UUID of the shared virsh secret used by Cinder to
# authenticate to the Ceph cluster.
cinder_rbd_secret_uuid: 'e80afa94-a64c-486c-9e34-d55e85f26406'

### The following parameter is used only if you are deploying Ceph
### as a backend for the Glance image service.

# The name of the pool used to store glance images.
glance_ceph_pool: 'images'

### The following parameters relate to Neutron L4-L7 services.

# A boolean specifying whether to enable the Neutron Load Balancing
# as a Service agent.
enable_lbaas: true

# A boolean specifying whether to enable the Neutron Firewall as
# a Service feature.
enable_fwaas: true

# A boolean specifying whether to enable the Neutron VPN as a
# Service feature.
enable_vpnaas: true

# An array of Neutron service plugins to enable.
service_plugins:
  - 'neutron.services.loadbalancer.plugin.LoadBalancerPlugin'
  - 'neutron.services.firewall.fwaas_plugin.FirewallPlugin'
  - 'neutron.services.vpn.plugin.VPNDriverPlugin'

# A hash of Neutron services to enable GUI support for in Horizon.
# enable_lb: Enables Neutron LBaaS agent support.
# enable_firewall: Enables Neutron FWaaS support.
# enable_vpn: Enables Neutron VPNaaS support
horizon_neutron_options:
  'enable_lb': true
  'enable_firewall': true
  'enable_vpn': true

# A boolean stating whether to run a "neutron-db-manage" on the
# nodes running neutron-server after installing packages.  In most
# cases this is not necessary and may cause problems if the database
# connection information is only located in the neutron.conf file
# rather than also being present in the Neturon plugin's conf file.
neutron_sync_db: false

## The following parameters are used to enable SSL endpoint support
# in keystone.

# Enable ssl in keystone config
enable_ssl: false

### NOTE: If enable_ssl is true, Replace the following lines
### with valid SSL certs. To generate you own self signed certs
### refer to the instructions from the following url
### https://help.ubuntu.com/12.04/serverguide/certificates-and-security.html
### After generating your certs, make sure the certs are copied
### to /etc/keystone/ssl/ on your control nodes and rerun puppet agent.
# SSL client certificate
ssl_certfile: '/etc/keystone/ssl/certs/keystone.pem'

# SSL certificate key
ssl_keyfile:  '/etc/keystone/ssl/private/keystonekey.pem'

# SSL CA Cert
ssl_ca_certs: '/etc/keystone/ssl/certs/ca.pem'

# SSL CA Key
ssl_ca_key: '/etc/keystone/ssl/private/cakey.pem'

# SSL cert subject
ssl_cert_subject: '/C=US/ST=Unset/L=Unset/O=Unset/CN=localhost'

## Cisco plugin settings
# Uncomment and adjust the lines below if you wish to use the Cisco
# Nexus plugin.  Make sure you also:
# 1.) Set the tenant_network_type (vlan or gre) in 
#     ../global_hiera_params/common.yaml
# 2.) Set the network_vlan_ranges to the vlan range you want to use
#     in hiera_data/tenant_network_type/vlan.yaml if using vlans
#     for tenant segmentation (as opposed to GRE).
# 3.) Add these two lines to ../class_groups/network_controller.yaml:
#     - "coe::nexus"
#     - "neutron::plugins::cisco"
# 4.) Uncomment the "Cisco plugin support" section of the
#     ../data_mappings/common.yaml file.
# 5.) Because the Ubuntu init scripts assume only one plugin will
#     be used, if you are using the Cisco plugin and OVS together
#     you may need to start neutron manually rather than via an init
#     script.  The command is:
#     /usr/bin/python /usr/bin/neutron-server --config-file \
#     /etc/neutron/neutron.conf --log-file /var/log/neutron/server.log \
#     --config-file /etc/neutron/plugins/cisco/cisco_plugins.ini \
#     --config-file /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini


# Use the Cisco plugin.  Note that we set this here rather than in
# global_hiera_params because in most use cases we'll want to set up
# both the Cisco plugin and the OVS plugin (and because the
# Cisco plugin by itself is agentless).
#neutron::core_plugin: 'neutron.plugins.cisco.network_plugin.PluginV2'

# The Nexus sub-plugin to use.
#nexus_plugin: 'neutron.plugins.cisco.nexus.cisco_nexus_plugin_v2.NexusPlugin'

# The vswitch sub-plugin to use with the Cisco plugin.
#vswitch_plugin: 'neutron.plugins.openvswitch.ovs_neutron_plugin.OVSNeutronPluginV2'

# The credentials to use to log into each Nexus switch in your topology.
# This should be an array of strings of the following form (switch IP
# address, username, and password separated by forward slashes):
# - 'switch1 IP address/username/password'
# - 'switch2 IP address/username/password'
#nexus_credentials:
#  - '172.18.117.27/admin/password'

# Switch configuration information.  This is a hash for each switch in
# your topology.  The format for each entry is:
#    'switch_name':    # The switch's hostname or other unique identifier
#    'ip_address': '192.168.2.81' # The IP of the switch
#    'username': 'admin'  # The username to log in with
#    'password': 'password' # The password to log in with
#    'ssh_port': 22  # The port NETCONF messages should be sent to via SSH
#    'servers':
#      'server1': '1/1'  # Hostname of a server and the port it is connected to
#      'server2': '1/2'
#
#nexus_config:
#  'cvf2-leaf-f1':
#    'ip_address': '172.18.117.27'
#    'username': 'admin'
#    'password': 'password'
#    'ssh_port': 22
#    'servers':
#      'control01': '1/7'
