
#### Build Node Cobbler Information ######## 

# If you use an HTTP/HTTPS proxy to reach the apt repositories that
# packages will be installed from during installation, uncomment this
# setting and specify the correct proxy URL.  If you do not use an
# HTTP/HTTPS proxy, leave this setting commented out.
#proxy: 'http://proxy-server.domain.com:8080'

#### Disk partitioning options ###########
# expert_disk is needed for fine-grained configuration of drive layout
# during Cobbler-driven installs. It is also required when installing
# on large drives that require GPT
expert_disk: true

# The /var directory is where logfiles and instance data are stored
# on disk.  If you wish to have /var on it's own partition (considered
# a best practice), set enable_var to true.
enable_var: true

# The Cinder volume service can make use of unallocated space within
# the "cinder-volumes" volume group to create iscsi volumes for 
# export to instances.  If you wish to leave free space for volumes
# and not preallocate the entire install drive, set enable_vol_space
# to true.
enable_vol_space: true

# Use the following two directives to set the size of the / and /var
# partitions, respectively.  The var_part_size directive will be ignored
# if enable_var is not set to true above.
root_part_size: 65536
var_part_size: 432000

# This domain name will be the name your build and compute nodes use for the
# local DNS.  It doesn't have to be the name of your corporate DNS - a local
# DNS server on the build node will serve addresses in this domain - but if
# it is, you can also add entries for the nodes in your corporate DNS
# environment they will be usable *if* the above addresses are routeable
# from elsewhere in your network.
domain_name: ctocllab.cisco.com

########### NTP Configuration ############
# Change this to the location of a time server or servers in your
# organization accessible to the build server.  The build server will
# synchronize with this time server, and will in turn function as the time
# server for your OpenStack nodes.
ntp_servers:
  - ntp.esl.cisco.com

# The time zone that clocks should be set to.  See /usr/share/zoneinfo
# for valid values, such as "UTC" and "US/Eastern".
time_zone: UTC

######### Node Addresses ##############
# Change the following to the short host name you have given your build node.
# This name should be in all lower case letters due to a Puppet limitation
# (refer to http://projects.puppetlabs.com/issues/1168).
build_node_name: buildpub2-1404

# Change the following to the host name you have given to your control
# node.  This name should be in all lower case letters due to a Puppet
# limitation (refer to http://projects.puppetlabs.com/issues/1168).
coe::base::controller_hostname: control-server

# The IP address to be used to connect to Horizon and external
# services on the control node.  In the compressed_ha or full_ha scenarios,
# this will be an address to be configured as a VIP on the HAProxy
# load balancers, not the address of the control node itself.
controller_public_address: 172.29.XX.XXX

# The protocol used to access API services on the control node.
# Can be 'http' or 'https'.
controller_public_protocol: 'http'

# The IP address used for internal communication with the control node.
# In the compressed_ha or full_ha scenarios, this will be an address
# to be configured as a VIP on the HAProxy load balancers, not the address
# of the control node itself.
controller_internal_address: 172.29.XX.XXX

# The IP address used for management functions (such as monitoring)
# on the control node.  In the compressed_ha or full_ha scenarios, this will
# be an address to be configured as a VIP on the HAProxy
# load balancers, not the address of the control node itself.
controller_admin_address: 172.29.XX.XXX

# Controller public url
controller_public_url: "http://172.29.XX.XXX:5000"

# Controller admin url
controller_admin_url: "http://172.29.XX.XXX:35357"

# Controller admin url
controller_internal_url: "http://172.29.XX.XXX:35357"

# Control node interfaces.
# internal_ip be used for the ovs local_ip setting for GRE tunnels.

# This sets the IP for the private(internal) interface of controller nodes
# (which is predefined already in $controller_node_internal, and the internal
# interface for compute nodes.  It is generally also the IP address
# used in Cobbler node definitions.
internal_ip: "%{ipaddress_p4p1}"

# The external_interface is used to provide a Layer2 path for
# the l3_agent external router interface.  It is expected that
# this interface be attached to an upstream device that provides
# a L3 router interface, with the default router configuration
# assuming that the first non "network" address in the external
# network IP subnet will be used as the default forwarding path
# if no more specific host routes are added.
external_interface: p4p2

# The public_interface will have an IP address reachable by
# all other nodes in the openstack cluster.  This address will
# be used for API Access, for the Horizon UI, and as an endpoint
# for the default GRE tunnel mechanism used in the OVS network
# configuration.
public_interface: p4p1

# The interface used for VM networking connectivity.  This will usually
# be set to the same interface as public_interface.
private_interface: p4p1

### Cobbler config
# The IP address of the node on which Cobbler will be installed and
# on which it will listen.
cobbler_node_ip: 172.29.XX.XXX

# The subnet address of the subnet on which Cobbler should serve DHCP
# addresses.
node_subnet: '172.29.XX.XXX'

# The netmask of the subnet on which Cobbler should serve DHCP addresses.
node_netmask: '255.255.255.224'

# The default gateway that should be provided to DHCP clients that acquire
# an address from Cobbler.
node_gateway: '172.29.XX.XXX'

# The admin username and crypted password used to authenticate to Cobbler.
admin_user: localadmin
password_crypted: $6$UfgWxrIv$k4KfzAEMqMg.fppmSOTd0usI4j6gfjs0962.JXsoJRWa5wMz8yQk4SfInn4.WZ3L/MCt5u.62tHDGB36EhiKF1

# Cobbler can instruct nodes being provisioned to start a Puppet agent
# immediately upon bootup.  This is generally desirable as it allows
# the node to immediately begin configuring itself upon bootup without
# further human intervention.  However, it may be useful for debugging
# purposes to prevent Puppet from starting automatically upon bootup.
# If you want Puppet to run automatically on bootup, set this to true.
# Otherwise, set it to false.
autostart_puppet: false

# Cobbler installs a minimal install of Ubuntu. Specify any additional
# packages which should be part of the base install on top of the minimal
# install set of packages
packages: 'openssh-server vim vlan lvm2 ntp'

# If you are using Cisco UCS servers managed by UCSM, set the port on
# which Cobbler should connect to UCSM in order to power nodes off and on.
# If set to 443, the connection will use SSL, which is generally
# desirable and is usually enabled on UCS systems.
ucsm_port: 443

# The name of the hard drive on which Cobbler should install the operating
# system.
install_drive: /dev/sda

# Set to 1 to enable ipv6 route advertisement.  Otherwise, comment out
# this line or set it to 0.
#ipv6_ra: 1

# Uncomment this line and set it to true if you want to use bonded
# ethernet interfaces.
#interface_bonding: true

# Use the following directive to enable promisc mode in cobbler preseed
# This may be needed in some HA installs to ensure that VRRP advertisements 
# to work.
enable_promisc_vrrp: false

# The IP address on which vncserver proxyclient should listen.
# This should generally be an address that is accessible via
# horizon.  You can set it to an actual IP address (e.g. "192.168.1.1"),
# or use facter to get the IP address assigned to a particular interface.
nova::compute::vncserver_proxyclient_address: "%{ipaddress_p4p1}"

# If you wish to customize the list of filters that the nova
# scheduler will use when scheduling instances, change the line
# below to be a comma-separated list of filters.  If set to false,
# the nova default filter list will be used.
# Example: 'RetryFilter,AvailabilityZoneFilter,RamFilter,
#           ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter'
nova::scheduler::filter::scheduler_default_filters: false

# The following is a set of arbitrary config entries to be
# created in nova.conf.  You can add arbitrary entries here
# that are not parameterized in the puppet-nova module for special
# use cases.
nova::config::nova_config:
  # Allow destination machien to match source for resize
  'DEFAULT/allow_resize_to_same_host':
    value: 'true'
  # Automatically confirm resizes after N seconds.  Set to 0 to disable.
  'DEFAULT/resize_confirm_window':
    value: '0'

### The following are passwords and usernames used for
### individual services.  You may wish to change the passwords below
### in order to better secure your installation.
cinder_db_password: cinder_pass
glance_db_password: glance_pass
keystone_db_password: key_pass
nova_db_password: nova_pass
network_db_password:   quantum_pass
database_root_password: mysql_pass
cinder_service_password: cinder_pass
glance_service_password: glance_pass
nova_service_password: nova_pass
ceilometer_service_password: ceilometer_pass
admin_password: Cisco123
admin_token: keystone_admin_token
network_service_password: quantum_pass
rpc_password: openstack_rabbit_password
metadata_shared_secret: metadata_shared_secret
horizon_secret_key: horizon_secret_key
ceilometer_metering_secret: ceilometer_metering_secret
ceilometer_db_password: ceilometer
heat_db_password: heat
heat_service_password: heat_pass
heat::engine::auth_encryption_key: 'notgood but just long enough i think'

# Set this parameter to use a single secret for the Horizon secret
# key, neutron agents, Nova API metadata proxies, swift hashes,etc.
# This prevents you from needing to specify individual secrets above,
# but has some security implications in that all services are using
# the same secret (creating more vulnerable services if it should be
# compromised).
secret_key: secret

# Set this parameter to use a single password for all the services above.
# This prevents you from needing to specify individual passwords above,
# but has some security implications in that all services are using
# the same password (creating more vulnerable services if it should be
# compromised).
password: password123

# Manage the Horizon vhost priority. Apache defaults to '25'. Here we
# set it to '10' so it is the default site. This allows Horizon to load
# at both it's vhost name and at the server's IP address.
horizon::wsgi::apache::priority: 10


### Swift configuration
# The username used to authenticate to the Swift cluster.
swift_service_password: swift_pass

# The password hash used to authenticate to the Swift cluster.
swift_hash: super_secret_swift_hash

# The IP address used by Swift on the control node to communicate with
# other members of the Swift cluster.  In the compressed_ha or full_ha
# scenarios, this will be the address to be configured as a VIP on 
# the HAProxy load balancers, not the address of an individual Swift node.
swift_internal_address: 192.168.242.41

# The IP address which external entities will use to connect to Swift,
# including clients wishing to upload or retrieve objects.  In the
# compressed_ha or full_ha scenarios, this will be the address to
# be configured as a VIP on the HAProxy load balancers, not the address
# of an individual Swift node.
swift_public_address: 192.168.242.41

# The IP address over which administrative traffic for the Swift
# cluster will flow.  In the compressed_ha or full_ha
# scenarios, this will be the address to be configured as a VIP on 
# the HAProxy load balancers, not the address of an individual Swift node.
swift_admin_address: 192.168.242.41

# The interface on which Swift will run data storage traffic.
# This should generally be a different interface than is used for
# management traffic to avoid congestion.
swift_storage_interface: eth0.222

# The IP address to be configured on the Swift storage interface.
swift_local_net_ip: "%{ipaddress_eth0_222}"

# The netmask to be configured on the Swift storage interface.
swift_storage_netmask: 255.255.255.0

# The IP address of the Swift proxy server. This is the address which
# is used for management, and is often on a separate network from
# swift_local_net_ip.
swift_proxy_net_ip: "%{ipaddress_eth0}"

### The following three parameters are only used if you are configuring
### Swift to serve as a backend for the Glance image service.

# Enable Glance to use Ceph RBD as it's backend storage by uncommenting
# the line below.  It can also be set to "file" or "swift".
#glance_backend: rbd

# The key used by Glance to connect to Swift.
glance::backend::swift::swift_store_key: secret_key

# The IP address to which Glance should connect in order to talk
# to the Swift cluster.
glance::backend::swift::swift_store_auth_address: '127.0.0.1'

# The volume name when using iSCSI for cinder
cinder_volumes_name: 'cinder-volumes'

### Ceph configuration
# The name of the Ceph cluster to be deployed.
ceph_cluster_name: 'ceph'

### Ceph configuration file
ceph_configuration_file: '/etc/ceph/ceph.conf'

# The FSID of the Ceph monitor node.  This should take the form
# of a UUID.
ceph_monitor_fsid: 'e80afa94-a64c-486c-9e34-d55e85f26406'

# The shared secret used to connect to the Ceph monitors.  This
# should be a crypted password.
ceph_monitor_secret: 'AQAJzNxR+PNRIRAA7yUp9hJJdWZ3PVz242Xjiw=='

# The short hostname (e.g. 'ceph-mon01', not 'ceph-mon01.domain.com') of
# the initial members of the Ceph monitor set.
mon_initial_members: 'ceph-mon01'

# The short hostname (e.g. 'ceph-mon01', no 'ceph-mon01.domain.com') of
# the primary monitor node.
ceph_primary_mon: 'ceph-mon01'

# The IP address used to connect to the primary monitor node.
ceph_monitor_address: '10.0.0.1'

# The rbd account OpenStack will use to communicate with ceph.
ceph_openstack_user: 'admin'

# Ceph will be deployed using the cephdeploy tool.  This tool requires
# a username and password to authenticate.
ceph_deploy_user: 'cephdeploy'
ceph_deploy_password: '9jfd29k9kd9'

# The name of the network interface used to connect to Ceph nodes.
# This interface will be used to pass traffic between Ceph nodes.
ceph_cluster_interface: 'eth1'

# The subnet on which Ceph intra-cluster traffic will be passed.
ceph_cluster_network: '10.0.0.0/24'

# The interface on which entities that want to import data into or
# extract data from the cluster will connect.
ceph_public_interface: 'eth1'

# The subnet on which external entities will connect to the Ceph cluster.
ceph_public_network: '10.0.0.0/24'

# A list of kernel modules that should be added to /etc/modules
# and thus auto-loaded into the kernel at boot time.  It is recommended
# that you include at least two modules: "8021q" is necessary for vlan
# tagging, and "vhost_net" increases networking performance of instances.
kernel_module_list:
  - 8021q
  - vhost_net

### The following four parameters are used only if you are configuring
### Ceph to be a backend for the Cinder volume service.

# Enable Cinder to use Ceph RBD as it's backend storage by uncommenting
# the line below.  It can also be set to 'iscsi'.
#cinder_backend: rbd

# The name of the pool used to store Cinder volumes.`
cinder_rbd_pool: 'volumes'

### The following parameter is used only if you are deploying Ceph
### as a backend for the Glance image service.

# The name of the pool used to store glance images.
glance_ceph_pool: 'images'

# Return the URL that references where the data is stored on
# the backend storage system.  For example, if using the
# file system store a URL of 'file:///path/to/image' will
# be returned to the user in the 'direct_url' meta-data field.
# This gives glance the ability to COW images, but revealing the
# storage location may be a security risk.
glance::api::show_image_direct_url: false

### The following parameters relate to Neutron L4-L7 services.

# A boolean specifying whether to enable the Neutron Load Balancing
# as a Service agent.
enable_lbaas: true

# A boolean specifying whether to enable the Neutron Firewall as
# a Service feature.
enable_fwaas: true

# A boolean specifying whether to enable the Neutron VPN as a
# Service feature.
enable_vpnaas: true

# An array of Neutron service plugins to enable.
service_plugins:
# For ML2, uncomment this line
  - 'neutron.services.l3_router.l3_router_plugin.L3RouterPlugin'
  - 'neutron.services.loadbalancer.plugin.LoadBalancerPlugin'
  - 'neutron.services.firewall.fwaas_plugin.FirewallPlugin'
  - 'neutron.services.vpn.plugin.VPNDriverPlugin'

# Set the interface driver
interface_driver: 'neutron.agent.linux.interface.OVSInterfaceDriver'

# ml2 hack. Current packages have ml2 enabled, even if you do not
# use the driver. This param corrects the configuration files.
# This is enabled by default. If you are using ml2, change to false.
disableml2: false

# Set the external network bridge.
# NOTE: If you change this, make sure to update the
# bridge mapping in tenant_network_type/*.yaml
external_network_bridge: 'br-ex'

# A hash of Neutron services to enable GUI support for in Horizon.
# enable_lb: Enables Neutron LBaaS agent support.
# enable_firewall: Enables Neutron FWaaS support.
# enable_vpn: Enables Neutron VPNaaS support
horizon_neutron_options:
  'enable_lb': true
  'enable_firewall': true
  'enable_vpn': true

# A boolean stating whether to run a "neutron-db-manage" on the
# nodes running neutron-server after installing packages.  In most
# cases this is not necessary and may cause problems if the database
# connection information is only located in the neutron.conf file
# rather than also being present in the Neturon plugin's conf file.
neutron_sync_db: false

## The following parameters are used to enable SSL endpoint support
# in keystone.

# Enable ssl in keystone config
enable_ssl: false

### NOTE: If enable_ssl is true, Replace the following lines
### with valid SSL certs. To generate you own self signed certs
### refer to the instructions from the following url
### https://help.ubuntu.com/12.04/serverguide/certificates-and-security.html
### After generating your certs, make sure the certs are copied
### to /etc/keystone/ssl/ on your control nodes and rerun puppet agent.
# SSL client certificate
ssl_certfile: '/etc/keystone/ssl/certs/keystone.pem'

# SSL certificate key
ssl_keyfile:  '/etc/keystone/ssl/private/keystonekey.pem'

# SSL CA Cert
ssl_ca_certs: '/etc/keystone/ssl/certs/ca.pem'

# SSL CA Key
ssl_ca_key: '/etc/keystone/ssl/private/cakey.pem'

# SSL cert subject
ssl_cert_subject: '/C=US/ST=Unset/L=Unset/O=Unset/CN=localhost'

# MySQL server options
#
# override_options is used to pass a hash of options to be set in
# /etc/mysql/my.cnf. The options are separated by the section of my.cnf
# to which they belong
#
# Options which need to be set in the mysqld section of my.cnf include:
#   bind-address: specifies the IP address on which the MySQL daemon listens
#   max-connections: specifies the number of simultaneous connections permitted
#   max_connect_errors: production deployments typically set this to 2^32-1
#
# In the isamchk section, key_buffer_size determines how much memory will be
# used to buffer index blocks for MyISAM tables
#
# Note that any other valid MySQL config file parameters can be added as
# needed by using this override_options mechanism. See override_options in
# https://github.com/puppetlabs/puppetlabs-mysql/tree/2.2.x#reference for more
# details on using this parameter with custom options.
mysql::server::override_options:
  mysqld:
    bind-address: 172.29.XX.XXX
    max-connections: 8192
    max_connect_errors: 4294967295
  isamchk:
    key_buffer_size: 64M
# if true, restart MySQL when config options are changed. true is appropriate
# for most installations
mysql::server::restart: true

## NFS Live migration options ##
# If on a debian-based distro, you must also set the nova uid/gid
# when using live migration. This is not needed for RHEL distros
# as they predefine the uid/gid.

# You can also manage the nova user's uid and gid. You will have to enable
# this in order to use NFS or Ceph live migrations. This option should NOT
# be used on RedHat platforms, as they already predefine the uid/gid for the
# nova user.
# nova::nova_user_id: '499'
# nova::nova_group_id: '499'

# migration_support: enable NFS mount handling.
# nfs_mount_path:  the path where nova instances are stored.
# nfs_mount_device: the full path to your NFS resource.
# nfs_fs_type: the mount type.
# nfs_mount_options: options, as they would appear in fstab
# Example config:
#  coe::compute::migration::migration_support: true
#  coe::compute::migration::nfs_mount_path: '/var/lib/nova/instances'
#  coe::compute::migration::nfs_mount_device: 'nfs.domain.com:/myinstances'
#  coe::compute::migration::nfs_fs_type: 'nfs'
#  coe::compute::migration::nfs_mount_options: 'auto'


## Cisco plugin settings
# Uncomment and adjust the lines below if you wish to use the Cisco
# Nexus plugin.  Make sure you also:
# 1.) Set the tenant_network_type (vlan or gre) in 
#     ../global_hiera_params/common.yaml
# 2.) Set the network_vlan_ranges to the vlan range you want to use
#     in hiera_data/tenant_network_type/vlan.yaml if using vlans
#     for tenant segmentation (as opposed to GRE).
#NOTE: For monolythic plugin mode follow these steps:
# 3.) Add these two lines to ../class_groups/network_controller.yaml:
#     - "coe::nexus"
#     - "neutron::plugins::cisco"
# 4.) Uncomment the "Cisco plugin support" section of the
#     ../data_mappings/common.yaml file.
# 5.) Because the Ubuntu init scripts assume only one plugin will
#     be used, if you are using the Cisco plugin and OVS together
#     you may need to start neutron manually rather than via an init
#     script.  The command is:
#     /usr/bin/python /usr/bin/neutron-server --config-file \
#     /etc/neutron/neutron.conf --log-file /var/log/neutron/server.log \
#     --config-file /etc/neutron/plugins/cisco/cisco_plugins.ini \
#     --config-file /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini

# Use the Cisco plugin.  Note that we set this here rather than in
# global_hiera_params because in most use cases we'll want to set up
# both the Cisco plugin and the OVS plugin (and because the
# Cisco plugin by itself is agentless).
#neutron::core_plugin: 'neutron.plugins.cisco.network_plugin.PluginV2'

# For ML2, uncomment this line
neutron::core_plugin: 'ml2'

# The Nexus sub-plugin to use.
#nexus_plugin: 'neutron.plugins.cisco.nexus.cisco_nexus_plugin_v2.NexusPlugin'

# The vswitch sub-plugin to use with the Cisco plugin.
#vswitch_plugin: 'neutron.plugins.openvswitch.ovs_neutron_plugin.OVSNeutronPluginV2'

# The credentials to use to log into each Nexus switch in your topology.
# This should be an array of strings of the following form (switch IP
# address, username, and password separated by forward slashes):
# - 'switch1 IP address/username/password'
# - 'switch2 IP address/username/password'
#nexus_credentials:
#  - '172.20.231.66/admin/c3l12345'


#NOTE: For ML2 support uncomment 
# If using ML2, set the mech drivers
ml2_mechanism_drivers: ['openvswitch', 'l2population', 'cisco_nexus']

# Switch configuration information.  This is a hash for each switch in
# your topology.  The format for each entry is:
#    'switch_name':    # The switch's hostname or other unique identifier
#    'ip_address': '192.168.2.81' # The IP of the switch
#    'username': 'admin'  # The username to log in with
#    'password': 'password' # The password to log in with
#    'ssh_port': 22  # The port NETCONF messages should be sent to via SSH
#    'servers':
#      'server1': '1/1'  # Hostname of a server and the port it is connected to
#      'server2': '1/2'
#
nexus_config:
  'nexus-3k-1':
    'ip_address': '172.20.XXX.XX'
    'username': 'admin'
    'password': '********'
    'ssh_port': 22
    'servers':
      'compute-server01': '1/6'
      'compute-server02': '1/7'
coe::base::supplemental_repo: false

